"""
X (Twitter) æ•°æ®æº
éœ€è¦ Bearer Token (æ¥è‡ª X Developer Portal)
"""
import json
import urllib.request
import ssl
from datetime import datetime
from typing import List, Dict, Any

from sources import BaseSource

class TwitterSource(BaseSource):
    """X/Twitter API v2 æ•°æ®æº"""
    
    BASE_URL = "https://api.twitter.com/2"
    
    def fetch(self) -> List[Dict[str, Any]]:
        if not self.is_enabled():
            return []
        
        bearer_token = self.config.get("bearer_token")
        query = self.config.get("query", "AI OR \"artificial intelligence\" -is:retweet")
        max_results = self.config.get("max_results", 10)
        
        if not bearer_token:
            print(f"  [X/Twitter Error] {self.name}: Missing bearer_token")
            return []
        
        try:
            # æœç´¢æœ€è¿‘æ¨æ–‡
            url = f"{self.BASE_URL}/tweets/search/recent"
            params = {
                "query": query,
                "max_results": min(max_results, 100),
                "tweet.fields": "created_at,public_metrics,author_id",
                "expansions": "author_id",
                "user.fields": "username,name"
            }
            
            # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
            query_string = "&".join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
            full_url = f"{url}?{query_string}"
            
            # è¯·æ±‚å¤´
            headers = {
                "Authorization": f"Bearer {bearer_token}",
                "User-Agent": "StellarPulse/1.0"
            }
            
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            
            req = urllib.request.Request(full_url, headers=headers)
            with urllib.request.urlopen(req, timeout=20, context=ctx) as resp:
                data = json.loads(resp.read().decode('utf-8'))
            
            # è§£ææ¨æ–‡
            items = []
            tweets = data.get('data', [])
            users = {u['id']: u for u in data.get('includes', {}).get('users', [])}
            
            for tweet in tweets:
                author_id = tweet.get('author_id')
                author = users.get(author_id, {})
                username = author.get('username', 'unknown')
                display_name = author.get('name', username)
                
                metrics = tweet.get('public_metrics', {})
                
                items.append({
                    "title": tweet.get('text', '')[:100] + "..." if len(tweet.get('text', '')) > 100 else tweet.get('text', ''),
                    "link": f"https://twitter.com/{username}/status/{tweet.get('id')}",
                    "summary": f"â¤ï¸ {metrics.get('like_count', 0)} | ğŸ” {metrics.get('retweet_count', 0)} | ğŸ’¬ {metrics.get('reply_count', 0)} | by @{username}",
                    "source": f"X/@{username}",
                    "pub_date": tweet.get('created_at', ''),
                    "fetched_at": datetime.now().isoformat(),
                    "raw_text": tweet.get('text', '')
                })
            
            return items
            
        except Exception as e:
            print(f"  [X/Twitter Error] {self.name}: {e}")
            return []


class TwitterSourceSimple(BaseSource):
    """ç®€åŒ–ç‰ˆ X/Twitter æ•°æ®æº (æ— éœ€APIï¼Œä½¿ç”¨Nitterç­‰é•œåƒ)"""
    
    def fetch(self) -> List[Dict[str, Any]]:
        """
        ç®€åŒ–å®ç°ï¼šé€šè¿‡ Nitter æˆ–å…¶ä»–é•œåƒè·å–å…¬å¼€æ¨æ–‡
        ä¸éœ€è¦ API Keyï¼Œä½†ç¨³å®šæ€§è¾ƒä½
        """
        if not self.is_enabled():
            return []
        
        # è·å–é…ç½®çš„æœç´¢è¯æˆ–ç”¨æˆ·ååˆ—è¡¨
        queries = self.config.get("queries", ["AI", "OpenAI", "SpaceX"])
        nitter_instance = self.config.get("nitter_instance", "https://nitter.net")
        
        items = []
        
        for query in queries[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡
            try:
                url = f"{nitter_instance}/search?f=tweets&q={urllib.parse.quote(query)}"
                
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE
                
                req = urllib.request.Request(
                    url,
                    headers={"User-Agent": "Mozilla/5.0"}
                )
                
                with urllib.request.urlopen(req, timeout=15, context=ctx) as resp:
                    html = resp.read().decode('utf-8')
                
                # ç®€å•è§£æ (Nitter HTMLç»“æ„)
                # æ³¨æ„ï¼šè¿™ä¾èµ–äºNitterçš„å…·ä½“å®ç°ï¼Œå¯èƒ½ä¸ç¨³å®š
                items.extend(self._parse_nitter_html(html, query))
                
            except Exception as e:
                print(f"  [Nitter Error] {query}: {e}")
        
        return items[:10]  # é™åˆ¶è¿”å›æ•°é‡
    
    def _parse_nitter_html(self, html: str, query: str) -> List[Dict]:
        """è§£æ Nitter HTML"""
        import re
        items = []
        
        # ç®€å•æ­£åˆ™åŒ¹é…æ¨æ–‡
        # æ ¼å¼: tweet-content ä¸­çš„æ–‡æœ¬
        tweet_pattern = r'<div class="tweet-content"[^>]*>.*?(<div class="tweet-body"[^>]*>.*?)</div>'
        tweets = re.findall(tweet_pattern, html, re.DOTALL)
        
        for tweet_html in tweets[:5]:
            try:
                # æå–ç”¨æˆ·å
                user_match = re.search(r'href="/([^"]+)"', tweet_html)
                username = user_match.group(1) if user_match else 'unknown'
                
                # æå–æ¨æ–‡å†…å®¹
                text_match = re.search(r'<div class="tweet-content media-body"[^>]*>(.*?)</div>', tweet_html, re.DOTALL)
                if text_match:
                    text = re.sub(r'<[^>]+>', '', text_match.group(1))
                    text = text.strip()[:200]
                    
                    items.append({
                        "title": text[:100] + "..." if len(text) > 100 else text,
                        "link": f"https://twitter.com/{username}",
                        "summary": f"Search: {query}",
                        "source": f"X/@{username}",
                        "pub_date": "",
                        "fetched_at": datetime.now().isoformat()
                    })
            except:
                continue
        
        return items
